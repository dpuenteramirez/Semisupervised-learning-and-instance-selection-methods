{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Â«Friedman TestÂ» from Demsar (2006)\n",
    "Adapted from cgosorio's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import scipy.stats\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind': \"{:6.3f}\".format})\n",
    "#np.set_printoptions(precision=4)\n",
    "RED='\\033[0;31m'\n",
    "GRN='\\033[0;32m'\n",
    "NC='\\033[0m'\n",
    "BLD='\\033[1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = 'f1-score'\n",
    "results = os.path.join('ranks', 'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base estimators used: ['KNeighborsClassifier' 'DecisionTreeClassifier' 'GaussianNB']\n",
      "Filters used: ['ENN' 'LSSm' 'ENANE' 'base']\n",
      "Percents labeled used: [0.05 0.1  0.15 0.2  0.25 0.3  0.35]\n",
      "# Datasets used: 18\n"
     ]
    }
   ],
   "source": [
    "filters = results_df['filter'].unique()\n",
    "base_estimators = results_df['base'].unique()\n",
    "percents_labeled = results_df['percent labeled'].unique()\n",
    "datasets = results_df['dataset'].unique()\n",
    "percents_labeled.sort()\n",
    "\n",
    "print(f\"Base estimators used: {base_estimators}\")\n",
    "print(f\"Filters used: {filters}\")\n",
    "print(f\"Percents labeled used: {percents_labeled}\")\n",
    "print(f\"# Datasets used: {len(datasets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of base estimator with filters:\n",
      " [['KNeighborsClassifier' 'ENN']\n",
      " ['KNeighborsClassifier' 'LSSm']\n",
      " ['KNeighborsClassifier' 'ENANE']\n",
      " ['KNeighborsClassifier' 'base']\n",
      " ['DecisionTreeClassifier' 'ENN']\n",
      " ['DecisionTreeClassifier' 'LSSm']\n",
      " ['DecisionTreeClassifier' 'ENANE']\n",
      " ['DecisionTreeClassifier' 'base']\n",
      " ['GaussianNB' 'ENN']\n",
      " ['GaussianNB' 'LSSm']\n",
      " ['GaussianNB' 'ENANE']\n",
      " ['GaussianNB' 'base']]\n"
     ]
    }
   ],
   "source": [
    "grouped_df = results_df.groupby(['dataset', 'percent labeled',\n",
    "                                              'base', 'filter']).mean()\n",
    "\n",
    "grouped_df = grouped_df[metric].to_frame()\n",
    "grouped_df.reset_index(inplace=True)\n",
    "\n",
    "bases_filters = np.array(list(product(base_estimators, filters)))\n",
    "print(\"Pairs of base estimator with filters:\\n\", bases_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    dataset  percent labeled                    base filter  \\\n",
      "0          BreastTissue.csv             0.05  DecisionTreeClassifier  ENANE   \n",
      "1          BreastTissue.csv             0.05  DecisionTreeClassifier    ENN   \n",
      "2          BreastTissue.csv             0.05  DecisionTreeClassifier   LSSm   \n",
      "3          BreastTissue.csv             0.05  DecisionTreeClassifier   base   \n",
      "4          BreastTissue.csv             0.05              GaussianNB  ENANE   \n",
      "...                     ...              ...                     ...    ...   \n",
      "1507  wifi-localization.csv             0.35              GaussianNB   base   \n",
      "1508  wifi-localization.csv             0.35    KNeighborsClassifier  ENANE   \n",
      "1509  wifi-localization.csv             0.35    KNeighborsClassifier    ENN   \n",
      "1510  wifi-localization.csv             0.35    KNeighborsClassifier   LSSm   \n",
      "1511  wifi-localization.csv             0.35    KNeighborsClassifier   base   \n",
      "\n",
      "      f1-score  \n",
      "0     0.183905  \n",
      "1     0.196561  \n",
      "2     0.197783  \n",
      "3     0.253685  \n",
      "4     0.098382  \n",
      "...        ...  \n",
      "1507  0.029355  \n",
      "1508  0.028805  \n",
      "1509  0.030231  \n",
      "1510  0.028359  \n",
      "1511  0.031792  \n",
      "\n",
      "[1512 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def base_filter_values():\n",
    "    values = pd.DataFrame()\n",
    "    for base_filter in bases_filters:\n",
    "        base_, filter_ = base_filter\n",
    "        working_df = grouped_df.loc[\n",
    "            (grouped_df['percent labeled'] == percent) &\n",
    "            (grouped_df['base'] == base_) &\n",
    "            (grouped_df['filter'] == filter_)\n",
    "            ]\n",
    "        value = working_df[metric].to_frame()\n",
    "        value.reset_index(inplace=True, drop=True)\n",
    "        value.columns = [':'.join(base_filter)]\n",
    "        values = pd.concat((value, values), axis=1)\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def split_onto_base_estimators():\n",
    "    for base_ in base_estimators:\n",
    "        base_dfs.append(curr_vals.filter(regex=base_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.05\n",
      "\n",
      "KNeighborsClassifier:base 2.47\n",
      "KNeighborsClassifier:ENANE 2.25\n",
      "KNeighborsClassifier:LSSm 2.67\n",
      "KNeighborsClassifier:ENN 2.61\n",
      "\n",
      "DecisionTreeClassifier:base 2.39\n",
      "DecisionTreeClassifier:ENANE 2.94\n",
      "DecisionTreeClassifier:LSSm 2.00\n",
      "DecisionTreeClassifier:ENN 2.67\n",
      "\n",
      "GaussianNB:base 2.22\n",
      "GaussianNB:ENANE 2.14\n",
      "GaussianNB:LSSm 2.39\n",
      "GaussianNB:ENN 3.25\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.1\n",
      "\n",
      "KNeighborsClassifier:base 2.78\n",
      "KNeighborsClassifier:ENANE 2.78\n",
      "KNeighborsClassifier:LSSm 2.22\n",
      "KNeighborsClassifier:ENN 2.22\n",
      "\n",
      "DecisionTreeClassifier:base 2.72\n",
      "DecisionTreeClassifier:ENANE 2.50\n",
      "DecisionTreeClassifier:LSSm 2.44\n",
      "DecisionTreeClassifier:ENN 2.33\n",
      "\n",
      "GaussianNB:base 3.17\n",
      "GaussianNB:ENANE 2.00\n",
      "GaussianNB:LSSm 2.17\n",
      "GaussianNB:ENN 2.67\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.15\n",
      "\n",
      "KNeighborsClassifier:base 2.50\n",
      "KNeighborsClassifier:ENANE 2.44\n",
      "KNeighborsClassifier:LSSm 2.78\n",
      "KNeighborsClassifier:ENN 2.28\n",
      "\n",
      "DecisionTreeClassifier:base 2.67\n",
      "DecisionTreeClassifier:ENANE 2.44\n",
      "DecisionTreeClassifier:LSSm 2.22\n",
      "DecisionTreeClassifier:ENN 2.67\n",
      "\n",
      "GaussianNB:base 2.39\n",
      "GaussianNB:ENANE 2.61\n",
      "GaussianNB:LSSm 2.44\n",
      "GaussianNB:ENN 2.56\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.2\n",
      "\n",
      "KNeighborsClassifier:base 2.33\n",
      "KNeighborsClassifier:ENANE 2.61\n",
      "KNeighborsClassifier:LSSm 2.83\n",
      "KNeighborsClassifier:ENN 2.22\n",
      "\n",
      "DecisionTreeClassifier:base 2.44\n",
      "DecisionTreeClassifier:ENANE 1.89\n",
      "DecisionTreeClassifier:LSSm 2.67\n",
      "DecisionTreeClassifier:ENN 3.00\n",
      "\n",
      "GaussianNB:base 2.28\n",
      "GaussianNB:ENANE 2.56\n",
      "GaussianNB:LSSm 2.67\n",
      "GaussianNB:ENN 2.50\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.25\n",
      "\n",
      "KNeighborsClassifier:base 2.44\n",
      "KNeighborsClassifier:ENANE 2.28\n",
      "KNeighborsClassifier:LSSm 2.61\n",
      "KNeighborsClassifier:ENN 2.67\n",
      "\n",
      "DecisionTreeClassifier:base 2.61\n",
      "DecisionTreeClassifier:ENANE 2.39\n",
      "DecisionTreeClassifier:LSSm 2.11\n",
      "DecisionTreeClassifier:ENN 2.89\n",
      "\n",
      "GaussianNB:base 2.44\n",
      "GaussianNB:ENANE 2.72\n",
      "GaussianNB:LSSm 2.11\n",
      "GaussianNB:ENN 2.72\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.3\n",
      "\n",
      "KNeighborsClassifier:base 2.28\n",
      "KNeighborsClassifier:ENANE 2.78\n",
      "KNeighborsClassifier:LSSm 2.50\n",
      "KNeighborsClassifier:ENN 2.44\n",
      "\n",
      "DecisionTreeClassifier:base 2.11\n",
      "DecisionTreeClassifier:ENANE 2.61\n",
      "DecisionTreeClassifier:LSSm 2.89\n",
      "DecisionTreeClassifier:ENN 2.39\n",
      "\n",
      "GaussianNB:base 2.78\n",
      "GaussianNB:ENANE 2.11\n",
      "GaussianNB:LSSm 2.89\n",
      "GaussianNB:ENN 2.22\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "> Percent labeled:  0.35\n",
      "\n",
      "KNeighborsClassifier:base 2.39\n",
      "KNeighborsClassifier:ENANE 2.36\n",
      "KNeighborsClassifier:LSSm 2.69\n",
      "KNeighborsClassifier:ENN 2.56\n",
      "\n",
      "DecisionTreeClassifier:base 2.28\n",
      "DecisionTreeClassifier:ENANE 2.72\n",
      "DecisionTreeClassifier:LSSm 2.56\n",
      "DecisionTreeClassifier:ENN 2.44\n",
      "\n",
      "GaussianNB:base 2.28\n",
      "GaussianNB:ENANE 2.50\n",
      "GaussianNB:LSSm 3.11\n",
      "GaussianNB:ENN 2.11\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for percent in percents_labeled:\n",
    "    curr_vals = base_filter_values()\n",
    "\n",
    "    base_dfs = list()\n",
    "    split_onto_base_estimators()\n",
    "    print('\\n\\n------------------------------------\\n\\n> Percent labeled: ',\n",
    "          percent)\n",
    "    for base, df in zip(base_estimators, base_dfs):\n",
    "        metric_vals = df.to_numpy()\n",
    "\n",
    "        N, k = len(datasets), len(df.columns)\n",
    "        assert N==metric_vals.shape[0] and k==metric_vals.shape[1]\n",
    "\n",
    "        rankings = scipy.stats.rankdata(-metric_vals, axis=1)\n",
    "\n",
    "        Rj = average_ranks = np.mean(rankings, axis=0)\n",
    "\n",
    "        average_rank_tuples = sorted(zip(df.columns, average_ranks), key=lambda\n",
    "            a: a[1])\n",
    "\n",
    "        average_rank_for = {method: value for method,\n",
    "                                              value in zip(df.columns,\n",
    "                                                           average_ranks)}\n",
    "        print()\n",
    "        for key, value in average_rank_for.items():\n",
    "            print(key, f'{value:.2f}')\n",
    "            \n",
    "            \n",
    "        part0 = (12*N)/(k*(k+1))\n",
    "        part1 = sum([Rj**2 for Rj in average_ranks])\n",
    "        part2 = (k*(k+1)**2)/4     \n",
    "        ð›˜2_F = part0*(part1-part2)\n",
    "        F_F = (N-1)*ð›˜2_F/(N*(k-1)-ð›˜2_F)\n",
    "        \n",
    "        data.append([percent, F_F, average_rank_for, N, k])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan And Davenport \n",
    "F_F = (N-1)*ð›˜2_F/(N*(k-1)-ð›˜2_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statology.org/f-distribution-calculator/\n",
    "\n",
    "With 3 classifiers, 3 filters and the base one, and 18 data sets, $F_F$ is distributed according to the $F$ distribution with\n",
    "$7-1=6$ and $(7-1)Ã—(18âˆ’1)=102$ degrees of freedom. The critical value of $F(6,102)$ for $\\alpha=0.05$\n",
    "is $2.00002$, so we reject the null-hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.359) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (1.84) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;32mWe reject the null-hypothesis, as the F_F value (3.15) is greater than the critical value (2.79)\n",
      "\u001b[1mThat means that there are statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (1.12) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.277) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;32mWe reject the null-hypothesis, as the F_F value (3.4) is greater than the critical value (2.79)\n",
      "\u001b[1mThat means that there are statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.452) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.475) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.106) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.814) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (2.56) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.277) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.321) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (1.19) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.906) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.452) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (1.19) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (1.71) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.25) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (0.364) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n",
      "\u001b[0;31mWe CAN NOT reject the null-hypothesis, as the F_F value (2.21) is less than the critical value (2.79)\n",
      "\u001b[1mThat means that there are not statistically differences between classifiers\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using critical values\n",
    "# FROM: https://stackoverflow.com/questions/39813470/f-test-with-python-finding-the-critical-value\n",
    "# SEE ALSO: https://www.statology.org/f-critical-value-python/\n",
    "alpha = 0.05\n",
    "for percent, F_F, average_rank_for, N, k in data:\n",
    "    critical_value = scipy.stats.f.ppf(q=1-alpha, dfn=(k-1), dfd=(k-1)*(N-1))\n",
    "\n",
    "    if F_F > critical_value:\n",
    "        print(f\"{GRN}We reject the null-hypothesis, as the F_F value ({F_F:.3}) is greater than the critical value ({critical_value:.3})\")\n",
    "        print(f\"{BLD}That means that there are statistically differences between classifiers{NC}\")\n",
    "    else:\n",
    "        print(f\"{RED}We CAN NOT reject the null-hypothesis, as the F_F value ({F_F:.3}) is less than the critical value ({critical_value:.3})\")\n",
    "        print(f\"{BLD}That means that there are not statistically differences between classifiers{NC}\")\n",
    "        #print(f\"Or that the test is not powerfull enough to detect the differences\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddbf58beb0ceb3f28487c8a8d7192b043471fe4d33849d20361912ddb46861bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('IS-SSL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
